{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20bdacb-a024-491e-b640-7e2987d4e6e8",
   "metadata": {},
   "source": [
    "###Exercise 1: Loading Data with Pandas\n",
    "\n",
    "1. Objective: Learn how to load and inspect datasets using Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5151a41e-4f2d-4eae-b4f1-b5471d48f778",
   "metadata": {},
   "source": [
    "2. Steps:\n",
    "\n",
    "Import the Pandas library and load a CSV file into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a799a008-0f51-4b46-8ca8-c5252300f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('heart_failure_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f1ac7-c535-4915-bb94-c73d3e64dd8a",
   "metadata": {},
   "source": [
    "Use the head(), tail(), and info() functions to inspect the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b629d687-85d9-49de-95cf-20a9f890224e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 299 entries, 0 to 298\n",
      "Data columns (total 13 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   age                       299 non-null    float64\n",
      " 1   anaemia                   299 non-null    int64  \n",
      " 2   creatinine_phosphokinase  299 non-null    int64  \n",
      " 3   diabetes                  299 non-null    int64  \n",
      " 4   ejection_fraction         299 non-null    int64  \n",
      " 5   high_blood_pressure       299 non-null    int64  \n",
      " 6   platelets                 299 non-null    float64\n",
      " 7   serum_creatinine          299 non-null    float64\n",
      " 8   serum_sodium              299 non-null    int64  \n",
      " 9   sex                       299 non-null    int64  \n",
      " 10  smoking                   299 non-null    int64  \n",
      " 11  time                      299 non-null    int64  \n",
      " 12  DEATH_EVENT               299 non-null    int64  \n",
      "dtypes: float64(3), int64(10)\n",
      "memory usage: 30.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "df.tail()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0847e4e-cc44-49e9-8114-7a4eec33505b",
   "metadata": {},
   "source": [
    "Check for missing values and data types of each column using isnull() and dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61f3143-7465-4d87-83ad-5aede9796451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                         float64\n",
       "anaemia                       int64\n",
       "creatinine_phosphokinase      int64\n",
       "diabetes                      int64\n",
       "ejection_fraction             int64\n",
       "high_blood_pressure           int64\n",
       "platelets                   float64\n",
       "serum_creatinine            float64\n",
       "serum_sodium                  int64\n",
       "sex                           int64\n",
       "smoking                       int64\n",
       "time                          int64\n",
       "DEATH_EVENT                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad57ef-17fc-442a-a83c-9db4219ef18f",
   "metadata": {},
   "source": [
    "How do you load a CSV file into a Pandas DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba9799-1335-46dc-ba1f-56363fbd7a8c",
   "metadata": {},
   "source": [
    "3. Questions:\n",
    "\n",
    "How do you load a CSV file into a Pandas DataFrame?\n",
    "You can use pd.read_csv('filename.csv') to load a CSV file into a DataFrame.\n",
    "\n",
    "What information does the info() function provide about the dataset?\n",
    "It shows column names, number of non-null entries, and the data types of each column.\n",
    "\n",
    "How can you identify missing values in the dataset?\n",
    "Use df.isnull().sum() to see how many missing values exist in each column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed3f398-f3d1-4ed7-a65f-a0e2c65e24b6",
   "metadata": {},
   "source": [
    "###Exercise 2: Handling Missing Data\n",
    "\n",
    "1. Objective: Practice techniques for handling missing data in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c89a0-2c1b-479a-ac02-dffc777cecf5",
   "metadata": {},
   "source": [
    "2. Steps:\n",
    "\n",
    "Identify missing values in the dataset using isnull().sum()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "327b3731-373f-4c25-87d1-112e462a298e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                         0\n",
       "anaemia                     0\n",
       "creatinine_phosphokinase    0\n",
       "diabetes                    0\n",
       "ejection_fraction           0\n",
       "high_blood_pressure         0\n",
       "platelets                   0\n",
       "serum_creatinine            0\n",
       "serum_sodium                0\n",
       "sex                         0\n",
       "smoking                     0\n",
       "time                        0\n",
       "DEATH_EVENT                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaa2a96-044b-42ae-82ea-f7bb07f85a38",
   "metadata": {},
   "source": [
    "Use different strategies to handle missing data:\n",
    "    Remove rows with missing values using dropna().\n",
    "    Fill missing values with the mean, median, or a specific value using fillna().\n",
    "    Use forward or backward filling (ffill() or bfill()) to fill missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc0583d-61ad-496b-b0a0-47ba447b3381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>anaemia</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>high_blood_pressure</th>\n",
       "      <th>platelets</th>\n",
       "      <th>serum_creatinine</th>\n",
       "      <th>serum_sodium</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoking</th>\n",
       "      <th>time</th>\n",
       "      <th>DEATH_EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>582</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>265000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7861</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>263358.03</td>\n",
       "      <td>1.1</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>162000.00</td>\n",
       "      <td>1.3</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>210000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>327000.00</td>\n",
       "      <td>2.7</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>155000.00</td>\n",
       "      <td>1.1</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1820</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>271</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2060</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>742000.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>278</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2413</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>140000.00</td>\n",
       "      <td>1.4</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>395000.00</td>\n",
       "      <td>1.6</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>299 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
       "0    75.0        0                       582         0                 20   \n",
       "1    55.0        0                      7861         0                 38   \n",
       "2    65.0        0                       146         0                 20   \n",
       "3    50.0        1                       111         0                 20   \n",
       "4    65.0        1                       160         1                 20   \n",
       "..    ...      ...                       ...       ...                ...   \n",
       "294  62.0        0                        61         1                 38   \n",
       "295  55.0        0                      1820         0                 38   \n",
       "296  45.0        0                      2060         1                 60   \n",
       "297  45.0        0                      2413         0                 38   \n",
       "298  50.0        0                       196         0                 45   \n",
       "\n",
       "     high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
       "0                      1  265000.00               1.9           130    1   \n",
       "1                      0  263358.03               1.1           136    1   \n",
       "2                      0  162000.00               1.3           129    1   \n",
       "3                      0  210000.00               1.9           137    1   \n",
       "4                      0  327000.00               2.7           116    0   \n",
       "..                   ...        ...               ...           ...  ...   \n",
       "294                    1  155000.00               1.1           143    1   \n",
       "295                    0  270000.00               1.2           139    0   \n",
       "296                    0  742000.00               0.8           138    0   \n",
       "297                    0  140000.00               1.4           140    1   \n",
       "298                    0  395000.00               1.6           136    1   \n",
       "\n",
       "     smoking  time  DEATH_EVENT  \n",
       "0          0     4            1  \n",
       "1          0     6            1  \n",
       "2          1     7            1  \n",
       "3          0     7            1  \n",
       "4          0     8            1  \n",
       "..       ...   ...          ...  \n",
       "294        1   270            0  \n",
       "295        0   271            0  \n",
       "296        0   278            0  \n",
       "297        1   280            0  \n",
       "298        1   285            0  \n",
       "\n",
       "[299 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "271e2c57-877b-4fd5-9964-d1f3362aff89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n",
      "       'ejection_fraction', 'high_blood_pressure', 'platelets',\n",
      "       'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time',\n",
      "       'DEATH_EVENT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62014639-93bc-4afc-8f39-7917fb3512e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = df['age'].fillna(df['age'].mean())\n",
    "df['age'] = df['age'].fillna(df['age'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9978bf46-2ef4-41c2-8449-b1b4b9fe872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='ffill')\n",
    "df.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7df1d7d-7dc3-4a0b-97a9-c401a2a39567",
   "metadata": {},
   "source": [
    "3. Questions:\n",
    "What strategy did you use to handle missing values, and why?\n",
    "It depends on the context, but common strategies include filling missing values with the mean for numerical data or using forward/backward fill for time-series data.\n",
    "\n",
    "How did filling missing values affect the dataset?\n",
    "Filling with statistical values (mean/median) retains the dataset size, while dropping rows with missing data reduces it.\n",
    "\n",
    "When might it be more appropriate to drop rows with missing values instead of filling them?\n",
    "When the percentage of missing data is small, or when filling the values would introduce bias or inaccuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b57f202-59f1-44a3-a34f-b097878dddef",
   "metadata": {},
   "source": [
    "###Exercise 3: Data Transformation\n",
    "\n",
    "1. Objective: Transform data to prepare it for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8c2776-b63f-4408-9bf0-185b000725eb",
   "metadata": {},
   "source": [
    "2. Steps:\n",
    "Normalize numerical features using Min-Max scaling or Z-score standardization with sklearn.preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "301337bf-8812-44c1-a206-b6cf07f5a39b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['column'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m      2\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m----> 3\u001b[0m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcolumn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['column'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df[['column']] = scaler.fit_transform(df[['column']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ed0ad-a47d-49ae-a0ad-576aa5b273bd",
   "metadata": {},
   "source": [
    "Encode categorical variables using one-hot encoding with pd.get_dummies() or sklearn.preprocessing.OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ed495-15b4-49ed-bc5c-b48f805ad960",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['category_column'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2ab7e2-32da-47c4-979d-2aaa2c65bebe",
   "metadata": {},
   "source": [
    "Use pd.cut() to bin continuous variables into discrete intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c589b23c-cb0c-4884-85f1-9fa5c9b1c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['binned'] = pd.cut(df['column'], bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eacb58b-72e3-4d44-9b7e-b4f2402e3d73",
   "metadata": {},
   "source": [
    "3. Questions:\n",
    "What is the difference between normalization and standardization?\n",
    "Normalization scales data between 0 and 1, while standardization transforms data to have a mean of 0 and standard deviation of 1.\n",
    "\n",
    "How does one-hot encoding transform categorical variables?\n",
    "It converts categorical columns into binary columns for each category, assigning a 1 where the category is present and 0 otherwise.\n",
    "\n",
    "Why might you want to bin continuous variables into categories?\n",
    "Binning can simplify data and make patterns more apparent, especially for models that don't assume linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa26ba-e42d-4094-8e88-78ae14eec69e",
   "metadata": {},
   "source": [
    "###Exercise 4: Feature Engineering \n",
    "\n",
    "1. Objective: Create new features to improve the predictive power of a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf4a871-21c5-4b10-8f77-bb976e63f3f7",
   "metadata": {},
   "source": [
    "2. Steps:\n",
    "Create new features by combining or transforming existing features (e.g., adding interaction terms or polynomial features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a1d70c-abfb-44b6-a781-6ba757a9a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['interaction_feature'] = df['feature1'] * df['feature2']  # Interaction term\n",
    "df['polynomial_feature'] = df['feature1'] ** 2  # Polynomial term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe28d1-c8d6-43e1-a243-3b0e92e7416d",
   "metadata": {},
   "source": [
    "Extract date-based features (e.g., year, month, day) from datetime columns using pd.to_datetime() and dt accessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62000ea7-39cf-42d0-9de1-fc4bc401a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_column'] = pd.to_datetime(df['date_column'])\n",
    "df['year'] = df['date_column'].dt.year\n",
    "df['month'] = df['date_column'].dt.month\n",
    "df['day'] = df['date_column'].dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01491e54-7ada-49cd-a0d9-b8be4f4c3aa4",
   "metadata": {},
   "source": [
    "Use domain knowledge to engineer features that might be useful for your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b6f404-9c5d-4025-a653-948ee7af1f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_weekend'] = df['date_column'].dt.weekday >= 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ea52d1-998d-43ff-9692-dbfb7bc86944",
   "metadata": {},
   "source": [
    "3. Questions:\n",
    "What new features did you create, and why?\n",
    "Interaction terms or polynomial features to capture relationships between variables, or date features to capture trends over time.\n",
    "\n",
    "How did the new features improve the dataset?\n",
    "New features can reveal relationships that improve model performance.\n",
    "\n",
    "How can date-based features be useful in a dataset?\n",
    "Date-based features help models understand seasonal trends, changes over time, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e9d3a-25ea-45e8-8035-b5d872d9e6a4",
   "metadata": {},
   "source": [
    "###Exercise 5: Data Cleaning###\n",
    "\n",
    "1. Objective: Clean data to ensure it's ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df675dc1-7be3-42ff-8587-b4200b755402",
   "metadata": {},
   "source": [
    "2. Steps:\n",
    "Remove duplicate rows using drop_duplicates()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc75887-abab-4255-aab9-1874feb97b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ffa215-5aa6-4b0c-bb90-b3bcc6fbb281",
   "metadata": {},
   "source": [
    "Detect and remove outliers using the Z-score method or the IQR method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a435122-eb2a-4b96-9493-d4dc3c310239",
   "metadata": {},
   "source": [
    "Z-score method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a539196-b8a8-4751-b2fa-17460b93ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "df = df[(np.abs(stats.zscore(df['column'])) < 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08787eff-6ebd-44cc-96e1-f7e15c63f076",
   "metadata": {},
   "source": [
    "IQR method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8765bbd6-0c12-4948-80d5-32cc12369a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['column'].quantile(0.25)\n",
    "Q3 = df['column'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df = df[~((df['column'] < (Q1 - 1.5 * IQR)) | (df['column'] > (Q3 + 1.5 * IQR)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673b3495-8805-4213-a1d9-3ddb46156617",
   "metadata": {},
   "source": [
    "Correct inconsistencies in categorical data (e.g., standardizing text formats or merging similar categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07c15bf-f206-46bd-82b8-5777205e4146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_column'] = df['category_column'].str.lower()\n",
    "df['category_column'].replace({'cat_a': 'category_a', 'cat_b': 'category_b'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b008a3-27f7-43f1-88cf-ce31926275f8",
   "metadata": {},
   "source": [
    "3. Questions:\n",
    "How did you identify and handle duplicate rows in the dataset?\n",
    "Use df.duplicated() to find duplicates and remove them with drop_duplicates().\n",
    "\n",
    "What method did you use to detect and remove outliers, and why?\n",
    "Z-score or IQR methods help identify values far from the mean that could distort model training.\n",
    "\n",
    "How did you address inconsistencies in categorical data?\n",
    "Standardized categories by converting all text to lowercase or merging similar categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5224652-a5de-437e-ad9e-7beffd407e64",
   "metadata": {},
   "source": [
    "###Exercise 6: Splitting Data into Training and Testing Sets\n",
    "\n",
    "1. Objective: Prepare the data for model training by splitting it into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116583bc-5800-4b99-aff3-89163d638b30",
   "metadata": {},
   "source": [
    "2. Steps:\n",
    "Use sklearn.model_selection.train_test_split() to split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a40b95-2854-4c33-a124-ae45a6b213c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop('target_column', axis=1)\n",
    "y = df['target_column']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd5ec2-0eb1-4019-a665-9d99176f8b49",
   "metadata": {},
   "source": [
    "Ensure that the target variable is correctly separated from the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adf598e-f9eb-4f13-94df-424ea75325b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "238bef56-6dbf-4ff4-8f6d-907abc71d2e1",
   "metadata": {},
   "source": [
    "Explore the impact of different train-test split ratios (e.g., 70-30, 80-20) on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b724b-7008-4e0c-bb3b-6ca37c775da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51730a97-9011-4f01-8c5b-4cddcb21e61d",
   "metadata": {},
   "source": [
    "3. Questions:\n",
    "How do you split a dataset into training and testing sets in Python?\n",
    "Use train_test_split() from sklearn to separate data into training and testing sets.\n",
    "\n",
    "What considerations should you keep in mind when choosing a train-test split ratio?\n",
    "The split should balance enough training data for learning and enough test data to evaluate generalization.\n",
    "\n",
    "How does the size of the training set impact the model's ability to generalize?\n",
    "A larger training set helps the model learn patterns but might leave less data to assess performance accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3bd899-c641-4d48-b398-4625b1af89f5",
   "metadata": {},
   "source": [
    "###Exercise 7: Data Preprocessing Pipeline\n",
    "\n",
    "1. Objective: Build a preprocessing pipeline to automate the data preparation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf5b78-4365-451f-b7f0-e233046840a7",
   "metadata": {},
   "source": [
    "2. Steps:\n",
    "Use sklearn.pipeline.Pipeline to create a pipeline that includes steps such as missing value imputation, feature scaling, and encoding categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ab531-01de-4b48-a84c-47f77afad098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numeric_features = ['num_feature1', 'num_feature2']\n",
    "categorical_features = ['cat_feature1']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f38ba-2a51-461b-9ecb-b3295afbcf9a",
   "metadata": {},
   "source": [
    "Fit the pipeline to the training data and transform the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386644b-65ac-44f4-9c07-bef6c605654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train)\n",
    "X_train_transformed = pipeline.transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30903c7-0c73-4d4c-ab92-27523557938f",
   "metadata": {},
   "source": [
    "Integrate the preprocessing pipeline with a machine learning model for end-to-end training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c2a09-0372-43e7-8656-8cfa5d9c0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', LogisticRegression())])\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20da682-3f2b-4999-8639-358d66e82a5c",
   "metadata": {},
   "source": [
    "3. Questions:\n",
    "What are the benefits of using a preprocessing pipeline?\n",
    "A pipeline streamlines the preprocessing steps and ensures consistency between training and testing data.\n",
    "\n",
    "How does the pipeline ensure consistency between training and test data transformations?\n",
    "It applies the same transformations (e.g., scaling, encoding) to both datasets.\n",
    "\n",
    "How can you extend the pipeline to include additional preprocessing steps?\n",
    "You can add steps for missing value handling, encoding, feature generation, etc., directly to the pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
